## Theory and Methodology

This chapter focuses on the methodology which went into preparing the data used in this thesis so that is was clean and in an appropriate format for analysis. This process took more or less a whole semester to complete. Moreover, Charlie Schufreider and I worked on this arduous task together, as the data are necessary and relevant to both of our theses. Thus due to our combined efforts on this portion of the project, this chapter is co-written with Charlie. His thesis focuses more specifically on the unique layout of this manuscript. This chapter will detail how we took the HMT's digital editions of the scholia, extracted the text, and dealt with the various obstacles that involve working with such a large and at times messy corpus. The two chapters which follow this one will then go into detial about the actual analysis of the data.

Even if the content of this thesis seems to focus strongly on the analysis of computer-generated data, the question which provides the basis for this thesis is one which has perplexed scholars of the Venetus A for centuries. Namely, there is a very hazy understanding of the source material for the scholia. Moreover, because the editions of the Alexandrian editors are not extant, it is not a simple task to piece together what scholia derive straight from Aristarchus' editions, what scholia are reworkings of his writings, and what scholia are from completely separate sources. The sheer amount of scholia in the Venetus A and the variety of different comments that are contained within these scholia adds to the difficulty of answering this question. In addition, there has been no consensus on how to interpret the unique five-zoned layout of the Venetus A scholia. As far back as 1875, the prominent Homeric scholar Karl Wilhelm Dindorf made a number of significant conclusions about different aspects of the scholia. In his introduction to his edition of the Venetus A, he concludes that while the intermarginal and main scholia are exceedingly similar in their subject-matter and word choice, they ultimately derive from different sources (vii). However, according to modern standards, his introduction lacks enough evidence to support so strong a conclusion. In his whole discussion of the nature of intermarginal and main scholia, he only mentions a handful of examples. In essence, he requires his readers to rely just on his ethos as a scholar. This is not to say that Dindorf's conclusions are incorrect, nor is it fair to say that Dindorf was conducting poor scholarship. It may be exceedingly possible that Dindorf had a wealth of examples to support his conclusion, but a print format prevented Dindorf from filling his introduction with too many examples. 

However, even if Dindorf had included every single piece of evidence he had collected to support his conclusions, his methodology would still put his findings under suspicion. Dindorf and nearly every other scholar prior to the rise of digital technologies relied on close-reading for any and all textual analyses. As Matthew Jockers notes in his book on the method of "distant-reading," close-reading is best applied to either individual texts or a small subset of them (19). Thus, something like a single poem lends itself well to a close-reading, but the entire corpus of British novels does not. It follows, then, that the estimated ten thousand scholia of the Venetus A manuscript constitute too large of a corpus for close-reading to be effective. Considering also that the scholia are an incredibly diverse set of documents, it would be unlikely that Dindorf or any other scholar could make many intelligible comments about minute linguistic patterns over so large and varied a corpus. Therefore, criticizing Dindorf or most of the previous Venetus A scholars is not quite fair, since they did not have the tools to analyze the text in a more systematic fashion.

Simply put, a computer is required for the "distant-reading" which the Venetus A scholia demands. Jockers defines this "distant-reading" as a sort of macroanalysis that, while devoid of "human synthesis and intuition," can process far more information and in much less time (19). Jockers' book details many examples of valid scholarly questions about literature that simply cannot be answered using close-reading alone. Typically, his examples deal with massive amounts of data, such as the entire corpus of nineteenth-century Irish literature. However, Jockers is by no means advocating for the abolition of close-reading in favor of his "distant-reading." Instead, he compares the relationship between close and distant reading to that of micro and macro economics. He writes, "Just as microeconomics offers important perspectives on the economy, so too does close-reading offer fundamentally important insights about literature" (26). Ultimately, he concludes, mining data from macroanalysis is not enough, but "human interpretation of the data... remains essential" (26). Thus, distant-reading is meant to complement close-reading, not replace it.

The number of avenues for pursuing such distant-reading continues to grow as the field of digital humanities itself continues to grow. A fuller description of the technologies applied in this thesis will be put forth later when it becomes more relevant. Still, it is worth mentioning the two primary digital methods: topic models and embedded word-vectors. 

Here, however, it is necessary to discuss the first step in any research where textual analysis is heavily reliant upon digital technologies. One must make the text or texts of interest machine-readable before any analysis can be carried out. For example, a concatenation of every non-white space character in the first line of Edgar Allen Poe's "The Raven" can be read by a human, albeit with some difficulty. On the other hand, this string of characters, "`Onceuponamidnightdreary,whileIpondered,weakandweary,`", is read by a computer as just that: a seemingly random collection of characters. Relying on digital techonologies to identify the meaningful word breaks in this example would certainly prove difficult, rendering the above string essentially useless for any textual analysis. While it seems unlikely that one might ever encounter such a concatenated text, texts are often transformed in unexpected ways when  one collects them for analysis from various webpages and outside sources. Scholars are not usually interested in analyzing texts which they themselves have created, but rather those which are available elsewhere, such as on an online archive. Thus importing an already-fashioned text requires a fair amount of careful prep work, or text wrangling, in order for it to suit one's analysis.

Luckily, in the case of this work on Iliadic scholia, the text of the scholia has already been written in a format that lends itself easily to digital analysis. This is because, as mentioned previously in chapter 1, the Homer Multitext project has been working for the last nine years to create digital editions of entire Iliad manuscripts. The scholia to the Venetus A manuscript have been transcribed using an XML format with separate editions for each book and type of scholia. By writing in XML, or Extensible Markup Language, the editors of the Homer Multitext project are able not only to record the visible letters on the manuscript page, but also to apply "markup" in order to supply more information about the text. Specifically, the Homer Multitext project follows the guidelines of the Text Encoding Initiative (TEI), a community of digital humanists who have established standards for editing texts. These guidelines inform the project's policies on creating digital editions such that the markup can be understood easily by other scholars. This extra information can convey how the scholia are ordered on the page, to which line of the poem a scholion refers, and whether a string of text in the scholion is a quotation from elsewhere in the *Iliad*. These and other more specific TEI elements will be explained as they become relevant. However, this section will focus on the structural aspects of the TEI guidelines as they relate to text wrangling.

The Homer Multitext project uses the same basic structural format for editing every scholion. That is, within one TEI element `<div>` there are three parallel subdivisions which each carry information about the scholion. The first subdivision contains just the lemma of the scholion, the second contains a uniform citation for the line of the *Iliad* on which the scholion is commenting, and the final subdivision contains the actual text content of the scholion itself. Such a logical structure is essential for the creation of a digital scholarly edition. However, this additional structural information hinders textual analysis. While something like `<div type="text"><l>Wrath, sing, oh goddess</l></div>` is a valid and logical XML transcription of the firt few words of the *Iliad*, clearly text like `<div>` and `<l>` should not be included in an analysis. If one were working with a small amount of scholia, it may make sense just to manually copy and paste all of the actual text from the XML files. The 8,000 scholia of this dataset, however, render manual extraction impractical. In order to avoid this labor-intensive process, we wrote a program using scala, a computer programming language, which was able to perform this extraction automatically.

Even with a complete collection of only the words from all the scholia, such a dataset would still be insufficient for any serious textual analysis if left unaltered. Homer Multitext policy dictates that every word of the manuscript be transcribed exactly as it appears on the manuscript page. This means that all orthographic variants and scribal errors exist in the Homer Multitext editions, cluttering up their data. Even their editing process can leave behind leftover markup that exists within the text content of the scholia. However, this leftover markup can easily be extracted from the text of the scholia. The markup is written in English and the scholia are written in Greek; thus one can simply filter out the letters of the English alphabet from the surrounding Greek to get a somewhat cleaner dataset. 

The problems which scribal errors and orthographic variants pose for data analysis are more complex and will receive more attention in the later discussion concerning the parsing of a text. For now, just an overview of what is meant by scribal errors and orthographic variants is required. The difference between these two is subtle, but important. Both refer to the misrepresentation of a word with respect to modern standards of Greek orthography. Put simply, a scribal error refers to the incorrect spelling or incorrect placement of a diacritical mark. An orthographic variant refers to the strict absence of an expected diacrtical mark. Ultimately, scribal errors are words rendered incorrectly. For example, a modern reader would expect the ancient Greek word for "wrath" to be written as μῆνις. If the scholiast were to add an additional letter to the word (μῆννις) or if he were to change the accentuation (μὴνις), these mutations would alter the very identity of the word and thus be considered scribal errors. Conversely, orthographic variants are equally valid forms of words, but do not follow modern orthographic standards. Instead, they follow standards which were accepted at the time of writing. Returning to the example of "μῆνις," if the scholiast were to leave out the accent mark from an otherwise correctly spelled word (μηνις), the lexical identity remains intact. Thus, for our purposes, "μῆνις" is orthographically equivalent to the unaccented "μηνις". How exactly such noise is handled within this corpus of Greek scholia will be discussed later, but it should be clear that the presence of alternative forms of the same word poses difficulties for textual analysis.

The inflected nature of the Greek language poses even more difficulties for textual analysis. The forms of Greek words change depending on their function within a sentence. Thus, the iota-sigma ending of "μῆνις" indicates that it is most likely the subject of a sentence or clause. "μῆνιν" still means wrath, however the iota-nu ending indicates that it is most likely the direct object of some verb. The inflection of verbs is even more complex than that of nouns and adjectives. A verb can have hundreds of different forms based on factors such as person, i.e. whether I, you, or he performs an action, or tense, i.e. whether an action is happening in the present, past, or future. Because of the various complexities that are inherent to Greek corpora, it is necessary to normalize the forms of Greek words so that all of the variants are accounted for. This need to normalize is not unique to the Greek language. When working with an English corpus, one might want "am," "is,"  "was," etc., to be identified as forms of the verb "to be." Thus, with respect to this Greek corpus, no matter the inflection of "μῆνις," it should always be identified as the same lexical entity. 

For English corpora, the process of normalization is relatively simple as most English words do not vary widely in their morphology. In the case of nouns, for example, pluralization is their only possible morphological change, and this change affects only their endings. The root, or stem, of a noun usually does not differ between singular and plural forms. Even in the slightly irregular case of "child/children," though the pluralization is not simply the addition of an "s," the stem "child" is unaltered. There are, of course, exceptions to this rule of fixed stems (mouse/mice, goose/geese), but largely this is a constant feature of English words. Even among more mutable parts of speech like verbs, stems largely remain constant in English morphology. As such, there are many programs which "stem" English texts automatically (snowball stemmer is one such program). The result is a version of the original text in which the morphological variants of each individual word are replaced with a single normalized form. For example, a simple stemmer would read in the sentence, “The men carried, the women carry, the boy carries, and the girl is carrying,” and replace each form of “carry” with a single, truncated form like “carri.” 

Due to the greater complexity of Greek morphology, such simple stemming processes are inappropriate for managing Greek corpora. The only factors which can change the stem of an English verb are tense and voice (i.e. whether the verb is active or passive). Active verbs are those whose subjects perform the action, whereas passive verbs are those whose subjects are acted upon. For example, the boy actively carries, while the man is passively carried. Tense and voice are the same factors which can change the stem of a Greek verb. Where the two differ, however, lies the number of principal parts and the number of possible inflected forms of those principal parts in each language. Verbs in English have at most three distinct principal parts and they often share the same stem across all three (e.g. carry, carried, carried). In Greek, most verbs have six principal parts, none of which are guaranteed to share the same stem. For example, “φέρω, οἴσω, ἤνεγκον, ἐνήνοκα, ἐνήνεγμαι, ἠνέχθην” are the principal parts of the verb "to carry." Furthermore the number of principal parts is not the only complicating factor for Greek verb morphology. In English, even an irregular verb such as “to go” has only five possible morphological variants: go, goes, going, went, and gone. Greek verbs, on the other hand, have far more than five possible forms, since each principal part supplies the stem for a myriad of forms that change based on person, number, and mood of the verb. Due to the wealth of diversity within Greek morphology, stemming is not an effective method for normalizing Greek corpora.

These problems can in part be solved by the more advanced process of parsing. This is because, unlike stemmers, a competent parser is able to recognize the lemma, or the dictionary form, of a word and give a detailed report of its exact form, i.e., in the case of a noun, its person, case, and number. Note, this use of "lemma" referring to a word's entry in a dictionary is different from the use of "lemma" to describe the words which identify to what part of the Iliad a scholion a refers. As opposed to a stem, which is the root of a word to which endings are added, a lemma is the dictionary form of a word. "Carries" would have a stem of "carri" but a lemma of "carry." Morpheus, a parsing tool created by the Perseus Project, satisfies the requirements for a competent Greek parser. To review, the ultimate purpose of using a parser at this stage in this research is the creation of a text that simplifies the morphological variants of words into single, normalized forms. Thus, using the Morpheus parser allows for the conversion of inflected Greek forms into their respective lemmata. By incorporating Morpheus into a scala script, a text suitabe for textual analysis is created. The created text simply replaces the words of the orignial text with their respective lemmata. Thus, no matter whether the original text had "μῆνιν" or "μῆνις," a completely parsed version would not differentiate between the two; both would be recorded as "μῆνις". 

If every word of the original of the text were able to be successfully parsed by Morpheus, there would be no need for further discussion about how to fashion a text for textual analysis. However, the previously discussed orthographic variants pose significant problems for analysis. To recap, an orthograhpic variant is a form of a word which is spelled correctly, but lacks a breathing mark, an accent, or some other diacritical mark. These variants often arise from the fact that the manuscript was written in the Byzantine period when the conventions for writing Greek may have been slightly different than in modern printed editions. So μῆνις and μηνις are considered Byzantine orthographic equivalents. These orthographic variants occur very frequently in the scholia of the Venetus A and are recorded exactly as they appear by the Homer Multitext project. As a result, these variants are so numerous that they cannot be ignored. 

Because of the abundance of these variants, the next step was to create a version of the scholia in which the Byzantine forms of words were replaced with their normalized forms. Charlie and I accomplished this by writing a script in scala which went through every word in all the scholia and checked it against the list of Byzantine orthagraphic equivalents, created by the HMT. If a word matched with one of the words on this list, then the script replaced it with the normalized form of the word. However, the process was slightly more complicated than this brief explanation. There are some words which, in their Byzantine forms, can stand in for the normalized versions of multiple words. This can be seen most obviously with the Byzantine form "η." This un-accented eta with no breathing could be several different Greek words. Because of the ambiguity that arises when the diacritics are left off and the word is viewed without context, a machine has no way of knowing what normalized word is its equivalent. This plain eta could be, among other things, a conjunction or an article. 

In order to account for this, when we first wrote the Byzantine orthographic normalized script, if a word could be multiple normalized words, then these normalized forms were grouped together with underscores in between them. For example, η appeared as "ἡ\_ἦ\_ἤ" in this normalized version of the scholia. However, this seemed to be a unique case. We realized that in most of these instances with multiple normalized forms, the actual lexcial identities of these forms were not different. That is, the difference between most of these forms was a grave accent versus an acute accent on the last syllable of a word. Because of this trivial difference, we modified the script so that when a word could be multiple normalized forms, it would be replaced with the first of these forms which were lexically identical. With the unique case of the word "ἡ/ἦ/ἤ," we also came to the conclusion that none of these words is particularly significant for analysis. Thus we also settled on having the plain letter eta be replaced by the first normalized form. Our goal in this whole process was to create a normalized version of the text that lent itself better to analysis. Thus what mattered most was a cleaner text. It is this version of the scholia, the "o-normalized" ("o" referring to the orthographic standards) which I use for much of my analyses in the following two chapters. There are also other versions of the scholia, such as the diplomatic, the p-normalized, and the m-normalized. The diplomatic version refers to the edition of thes scholia which contains what was recorded by the HMT as being exactly what is written on the page of the manuscript. The p-normalized version of the scholia is normalized based on paleographic standards. That is, in the case of an abbreviation, the diplomatic version contains the abbreviation, and the p-normalized the expanded form of the word. Lastly, the m-normalized version of the scholia is normalized based on morphological standards. While all of these editions are important and useful for different types of analysis, the o-normalized is the most useful for my purposes. Thus it has received the most explanation in this section. 

After this lengthy process of text wrangling, at last there exists an edition (or rather, multiple editions) of the text which is able to be analyzed through macroanalysis. That distant-reading which Matthew Jockers describes at length in his book can finally be applied to the scholia. The next chapter will detail the specific methods within macroanalysis that were employed to elucidate hidden patterns of language within the scholia. The hope is that topic models will shed light upon that question which began this chapter concerning the the source material for the scholia of the Venetus A. The point to take away from this chapter is that digital corpora are often very messy and it is difficult to prepare them so that they can be sufficiently analyzed. At least for this thesis, an entire semester was spent writing and debugging scala scripts in order to create a usable text for analysis. Even then, there is still more work to be done in order to make the data as clean as possible. For it is a fact that cleaner data will at the very least produce results of greater significance. So although it may seem like a hassle, this data preparation is the most important step in any digital humanities work.
